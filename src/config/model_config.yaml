# Common model settings
model:
  name: "meta-llama/Llama-2-7b-hf"
  dtype: "bfloat16"
  cache_dir: "path/to/cache"
  trust_remote_code: true
  tensor_parallel_size: 1

# Generation settings
sampling_params:
  temperature: 0.7
  top_p: 1.0
  max_tokens: 300

cot_generation:
  batch_size: 8
  num_iterations: 8

# Self-evaluation settings
self_eval:
  S: 5
  N: 8  # Number of explanations to consider

# Reward model training settings
training:
  per_device_train_batch_size: 32
  num_train_epochs: 10
  learning_rate: 3.0e-7
  weight_decay: 0.01